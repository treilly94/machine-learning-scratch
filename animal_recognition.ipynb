{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to crop images to same size\n",
    "def crop_center(img,cropx,cropy):\n",
    "    y,x = img.shape\n",
    "    startx = x//2-(cropx//2)\n",
    "    starty = y//2-(cropy//2)    \n",
    "    return img[starty:starty+cropy,startx:startx+cropx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_names = [\"cat\", \"dog\"]\n",
    "# Target\n",
    "y = []\n",
    "# Data\n",
    "X = None\n",
    "# Loop through all .jpg files in a directory\n",
    "for fn in os.listdir(\"./animals/\"):\n",
    "    if fn.endswith(\".jpg\"):\n",
    "        # Get image \n",
    "        im = Image.open(\"./animals/\" + fn).convert(\"L\")\n",
    "        # Crop image\n",
    "        half_the_width = im.size[0] / 2\n",
    "        half_the_height = im.size[1] / 2\n",
    "        im = im.crop(\n",
    "            (\n",
    "                half_the_width - 50,\n",
    "                half_the_height - 75,\n",
    "                half_the_width + 50,\n",
    "                half_the_height + 75\n",
    "            )\n",
    "        )\n",
    "        im = list(im.getdata())\n",
    "        # Read images into arrays in data\n",
    "        # Get first\n",
    "        if X is None:\n",
    "            X = np.array(im)\n",
    "        else:\n",
    "            X = np.vstack((X, im))\n",
    "        # Get targets from file name\n",
    "        fn = fn.partition(\"-\")\n",
    "        y.append(target_names.index(fn[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 15000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[191, 188, 186, ..., 172, 167, 154],\n",
       "       [134, 149, 133, ..., 184, 190, 184],\n",
       "       [255, 255, 255, ..., 147, 148, 166],\n",
       "       ...,\n",
       "       [255, 255, 255, ..., 172, 191, 204],\n",
       "       [123, 129, 141, ..., 157, 193, 202],\n",
       "       [242, 242, 242, ..., 192, 187, 185]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 15000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 150\n",
    "\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized', whiten=True).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-78b6e6a4eb82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meigenanimals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'h' is not defined"
     ]
    }
   ],
   "source": [
    "eigenanimals = pca.components_.reshape((n_components, h, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a SCM classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model on the test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-08d41713f6c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/ml_face_recognition/venv/lib64/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3100\u001b[0m                         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3101\u001b[0m                         \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3102\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   3103\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3104\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/ml_face_recognition/venv/lib64/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1845\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/Documents/Projects/ml_face_recognition/venv/lib64/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5471\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5473\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5474\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/ml_face_recognition/venv/lib64/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    651\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    652\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 518.4x1382.4 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert arrays to images and show\n",
    "plt.figure(figsize=(1.8 * 4, 2.4 * len(X)))\n",
    "plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "for i in range(len(X)):\n",
    "    plt.subplot(len(X), 4, i + 1)\n",
    "    plt.imshow(X[i])\n",
    "    plt.title(target_names[y[i]])\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
